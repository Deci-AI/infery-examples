{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Inspection Using Infery\n",
    "\n",
    "  Infery's main objective is to provide a simple, fast Python API for inference across different platform and deep learning frameworks. However, to support Deci's researchers' growing needs, Infery has also accumulated multiple useful features for inspecting models and analyzing their architecture. In this notebook we focus on two of those features - *Netron integration* and *layer profiling*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Netron Integration\n",
    "\n",
    " Netron is a viewer for neural network, deep learning and machine learning models. It supports viewing models in ONNX, TensorFlow Lite, Caffe, Keras, ncnn, MNN, Core ML, and more. It also provides a Python package for starting a Netron session from within your code.\n",
    " Utilizing this package, Infery allows developers view their LoadedModel with a single line of code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import infery\n",
    "\n",
    "# Create an Infery LoadedModel object\n",
    "onnx_model_path = \"../../models/resnet18_batchsize_64.onnx\"\n",
    "onnx_model = infery.load(model_path=onnx_model_path, framework_type=\"onnx\")\n",
    "\n",
    "# View the LoadedModel through Netron. Any input will stop serving the Netron viewer on the opened 8080 localhost port.\n",
    "onnx_model.open_in_netron()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Profiling an ONNX Model\n",
    "\n",
    "Infery also provides layer-level profiling capabilities. By simply loading a model with a `profiling=True` to Infery's `load` method, the returned LoadedModel may be used to generate Pandas DataFrames containing the time spent in each layer and the percentage of the total inference time that the layer took - simply call `get_layers_profile_dataframe`. Finally, the X top bottlenecks of the model may be fetched by passing X the `get_bottlenecks` method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an Infery LoadedModel object\n",
    "onnx_model_path = \"../../models/resnet18_batchsize_64.onnx\"\n",
    "onnx_model = infery.load(\n",
    "    model_path=onnx_model_path, framework_type=\"onnx\", profiling=True\n",
    ")\n",
    "\n",
    "# Get timing DataFrame and the top 10 model bottlenecks (10 layers that take the longest to run)\n",
    "profiling_dataframe = onnx_model.get_layers_profile_dataframe()\n",
    "bottlenecks = onnx_model.get_bottlenecks(num_layers=10)\n",
    "\n",
    "# Print the fetched 15 bottlenecks\n",
    "print(bottlenecks.to_string())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And a simple example of visualizing the percentage and total time spent in the fetched bottlenecks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot total time (ms) spend in each layer\n",
    "ax = bottlenecks.plot.bar(\n",
    "    x=\"Layer Name\", y=\"ms\", rot=90, title=\"ResNet18 ONNX Bottlenecks - Total Time\"\n",
    ")\n",
    "ax.set_xlabel(\"Layer Name\")\n",
    "ax.set_ylabel(\"Inference Time [ms]\")\n",
    "\n",
    "# Plot total time (ms) spend in each layer\n",
    "ax = bottlenecks.plot.bar(\n",
    "    x=\"Layer Name\",\n",
    "    y=\"Percentage\",\n",
    "    rot=90,\n",
    "    title=\"ResNet18 O NNX Bottlenecks - Percentage\",\n",
    ")\n",
    "ax.set_xlabel(\"Layer Name\")\n",
    "ax.set_ylabel(\"Percentage of Total Inference Time [%]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Profiling a TensorRT Model\n",
    "\n",
    "And a similar example of profiling a ResNet18, but this time after it has been compiled to TensorRT (notice this requires and Nvidia GPU compatible with RTX 3070 engines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an Infery LoadedModel object\n",
    "onnx_model_path = \"../../models/resnet18_batchsize_64_RTX3070.pkl\"\n",
    "onnx_model = infery.load(\n",
    "    model_path=onnx_model_path, framework_type=\"trt\", profiling=True\n",
    ")\n",
    "\n",
    "# Get timing DataFrame and the top 10 model bottlenecks (10 layers that take the longest to run)\n",
    "bottlenecks = onnx_model.get_bottlenecks(num_layers=10)\n",
    "\n",
    "ax = bottlenecks.plot.bar(\n",
    "    x=\"Layer Name\", y=\"ms\", rot=90, title=\"ResNet18 ONNX Bottlenecks - Total Time\"\n",
    ")\n",
    "ax.set_xlabel(\"Layer Name\")\n",
    "ax.set_ylabel(\"Inference Time [ms]\")\n",
    "\n",
    "# Plot total time (ms) spend in each layer\n",
    "ax = bottlenecks.plot.bar(\n",
    "    x=\"Layer Name\",\n",
    "    y=\"Percentage\",\n",
    "    rot=90,\n",
    "    title=\"ResNet18 ONNX Bottlenecks - Percentage\",\n",
    ")\n",
    "ax.set_xlabel(\"Layer Name\")\n",
    "ax.set_ylabel(\"Percentage of Total Inference Time [%]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enjoy! :)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}